Bien, entonces recién hicimos una revista sobre IPv6, hablamos de tunneling, hablamos de cómo es el autoadresamiento que hace la configuración automática de adreses que hace IPv6 y algunos otros aspectos del addressing de IPv6. Nos estamos por despedir la capa 3, pero no podemos hacerlo sin antes ver, tal cual en IPv4 e ICMP en 6 es, como bien sabemos, es un protocolo que está a la misma altura en la capa de red y que ayuda a ICMP de 6. Vamos, es fundamentalmente, semánticamente, muy muy parecido a IPv4 y tiene unas pequeñas diferencias que son las que recalcaremos especialmente a Albert Lurka. Tiene el mismo diseño de protocolo si se quiere, recordemos que en ICMP 4 teníamos el tipo de mensaje y después el código de mensaje. Lo que ven acá en negrita, como "destination unreachable", "packet to bit", es el tipo de mensaje y después esto tiene un subcódigo que vendría a ser el subtipo de mensaje, lo que está aquí adentro. Fíjense que los conceptos son muy parecidos a 4, "destination unreachable" tenemos "no root to destination", que en 4 sería "network destination unreachable". Acá aparecen este tipo de cosas, "beyond scope of source address", ¿alguien me traduce qué significa esto con lo que acabamos de ver? ¿Qué es "source destination"? ¿Qué será eso? Esto lo responde un router y dice "no, destination unreachable, no jodas, de esa manera no podés ir". ¿Qué me está diciendo? Un F80, un 2000... Claro, exacto, es decir, yo he intentado, que es muy parecido a lo que tenemos en la cloud, lo que pasa es que el stack tiene que ser inteligente para decir "ok, mi default router es F80, pero si yo voy a un destino 2000, no puedo ir con una source address F80, tengo que seleccionar", y hay todo un algoritmo para seleccionar la source address adecuada para saber que el regreso sea correcto. Entonces, protocolo IPv6 es como que ya tienen bebido en el mismo stack estos scopes distintos, entonces yo no puedo contactar un scope global desde un scope local, sencillamente porque no funciona y porque no hay NAT en 6. En 4 lo podemos hacer alegremente, que nosotros vamos de 192, 168, Sarasasa, hasta, no sé, 200.1.2.3, el protocolo nacional al respecto, porque no había en IPv4 esta gran separación de scopes. Hay direcciones privadas, existen direcciones privadas en IPv4, existe NAT en IPv4, pero en IPv6 no existe NAT, por lo tanto es inválido intentar desde una link local a una global. Entonces, directamente tu router acá te va a explotar, si es que no te explota el stack local posiblemente. Ahí yo no recuerdo los detalles. Las mismas cosas que habíamos visto, address unreachable le llama acá host destination unreachable, porque el destino no está presente en el link, como sabemos el único que puede dar host destination unreachable y asegurar que no está el host es el último router que ya es vecino de mi destino. Y hay algunas otras cosas, un poco más, que acá aparece más explícito un caso de firewall, que es algo parecido a esto, la política de ingreso/degreso no te permite. Y hay un específico para reject también, cuando vos tenés entrada local en la tabla de routeo que directamente te recheca. Pero básicamente, conceptualmente es destination unreachable, más allá de que aparece este caso particular que les mencioné recién. PMTU packet tweak, directamente, acá no hay "uh, debería fragmentarte, pero DF=1" me dijiste, no, recordemos que no hay fragmentación en camino, en el camino los routers no fragmentan, así que acá todos tienen que ser señoritos que descubren MTU, entonces si vos, origen, me mandaste un paquete y yo link próximo no pasa, listo. Packet tweak, es como que el DF=1 está implícito, porque de hecho, como hemos visto, no hay un renglón, no hay un lugar para especificar fragmentación en IPv6. El time exceed, que es el mismo que vimos, nada más que en el caso de 6 hemos visto que ya se le llama adecuadamente a la cabecera, lo que mal llamábamos en IPv4, time to leave, como no es un time, bien lo hemos hablado mucho, ahora es un hop limit, que es el correcto. Básicamente lo que siempre hemos dicho coloquialmente es "te acabó la nafta". Parameter problem es lo mismo que en IPv4, no sabés escribir IPv6, es básicamente vos escribiste algo en la cabecera que está mal, aprendés a escribir IPv6, entonces SMP en algunos casos devuelve el parameter problem, o alguno de los campos de la cabecera no está correcto. Y acá tenemos los bien conocidos, el core request y core reply. Router advertisement es nuevo, ¿me recuerdan qué era lo que hace el router advertisement? Lo acaban de ver hace un ratito con Diego. Router advertisement. Pasa del prefijo de red. ¿Quién lo envía eso? El gateway. Claro, los routers, básicamente "llenan", entre comillas, "llenan" la red de router advertisement, ¿y qué hacen los nodos con esos routers advertisement? Fijan su OIP. Tomando justamente el prefijo ese que defaulteamos en 64 y le pegan el POS fijo, el lado host, que está construido determinísticamente en la MAC, para obtener la IPv6 de 128 completa. Entonces eso es un concepto súper importante que no existe en IPv6. En realidad no existe un mecanismo, no. No existe un mecanismo similar. Router solicitation, esto en realidad es el complemento de esto. En principio los nodos, ustedes van a ver que también tienen el permiso, si se quiere, de poder pedir, cuando recién arranca y dice, "quiero IPv6" y puede que el router advertisement esté puesto con una frecuencia muy grande, por decir, en vez de ser cada, creo que nosotros lo tenemos que ver bastante rápido, cada 30 segundos, cosas que se auto-configuran solitos, el admin puede decir, bueno, cada 5 minutos, este nodo acaba de arrancar y ya quiere, necesita IPv6, entonces apenas arranca y obtiene el link, hace un router solicitation. Obtiene link, pero ¿con qué IP va a hacer un router solicitation si solamente tiene link? ¿Nadie le ha dado un IP? Recapitulo. El nodo final, el host, obtiene link y dice, "che, yo quiero empezar a andar en IPv6, vamos a ver, capaz que tenga un IPv6 público, entonces hago un router solicitation, pero ¿cómo voy a hacer un router solicitation si todavía no tengo IPv6? Solamente obtener link. ¿Qué hay de mentiroso en todo eso que acabo de decir? Hay una parte que es mentirosa, que es mentira. Y si tiene porque el F80 lo podés asignar solo. Exacto, tal cual, yo tengo link y ya tengo IPv6, no tengo que pedirle a nadie más permiso a nada, por protocolo, yo tengo link, tengo la MAC, si, la parte de atrás, F80, MAC, bling, ya tengo IPv6, suficiente para esta cacerola. Hago un router solicitation justamente para descubrir, a ver, che, acá hay un router que nos saque de esta cacerola. Entonces, contra el router solicitation viene un router advertisement. Y si no, si no estoy tan frenético, me pongo a escuchar y vendrá un router advertisement y le pego otros prefijos, además del prefijo F80, que sabemos que tiene scope de cacerola, de enlace. Bien. Acá está el más interesante distinto, que es el network solicitation y el network advertisement. ¿Qué serán esos? Si tuviéramos que compararlos con IPv4. Solicitud de vecinos, advertisement de vecinos. [VOCES INTERPUESTAS] Cuando estudiamos la clase pasada, sí, usa multicast, dijimos que había un protocolo que notablemente se despedía y no existía ni pero 6. Protocolo fundamental para IP en un momento que necesita algo y después es olvidable, pero siempre tiene que estar, porque si no, eh. Y que no es IP el protocolo. [VOCES INTERPUESTAS] [VOCES INTERPUESTAS] Es el ARP, ¿no? El ARP. ¿Qué problema resolvía ARP en 4? Perdón, en 6 no existe ARP. Con una IP solicitaba la MAC. Mapa direcciones de capa 3 a capa 2, ¿no? Eso es lo que-- IPv6 lo que trae como novedad, si quieres, es que no le pide ayuda a un protocolo distinto a los que ya tiene, sino que usa el mismo, IPv6 sobre IPv6 para hacer esa resolución. Y vos te rascarás la cabeza y decís, pará, pero estás pidiendo resolución de vecino de IPv6 y estás usando el mismo IPv6. Bueno, justamente para estos mecanismos de multicast, recordemos, podría ser broadcast, pero broadcast no se usa en IPv6, entonces usamos multicast, con el cual básicamente es conceptualmente la misma implementación de ARP. Te voy a preguntar en multicast cuál es la MAC de este IPv6. Nuevamente, digo multicast, bien podría decir broadcast como ARP, pero como esta cosa evolucionó y se encontró que broadcast realmente es malo, se usa multicast para decir, che, ¿quién tiene este IPv6 en la vecindad? Y el vecino responderá el mecanismo del mismo, pero lo que es muy interesante es que la implementación es como parte integral del stack, o mejor dicho, la capa de stack IPv6 está todo autorresuelto. Con el caso de IPv4, IPv4 necesitaba pedirle un protocolo distinto, porque ARP en principio no tiene que ver con IP, es un protocolo de ayuda, pero tiene que estar ahí para poder hacer que IPv4 funcione. En IPv6 está todo autorresuelto, ¿no? Eso es importante y esa es la gran diferencia, una de las grandes diferencias que tiene ICMPv6 de ICMPv4. Y creo que estamos, sí, señor. Bueno, entonces, como habíamos dicho, y voy a aprovechar, estoy compartiendo esa ventana, déjenme buscar la presentación de TCP. Vamos a cambiar bruscamente de capa y nos vamos a ir para arriba. Como les comentaba Diego, hemos decidido, hemos decidido este año, para que no se nos escape del tintero, dar IPv6 antes de TCP. En años pasados nos parecía más, ¿puedes creer hasta el control? Sí, el control de esta porquería es poco reliable, pero estúpido sistema operativo del reverendo. ¿Pip? Sí, pip. No, me tengo que dejar. Qué cosa insoportable, por favor. Sacarte una SIM para ahí, para despuntar el vicio ahí. La tengo al lado, la tengo al lado. ¿Sabes cuál es el problema? Me es complicado, esta Mac la tengo guaireada para los tres monitores que tengo acá. Y las tengo por atrás, me da despedir sacar el HDMI y ponérselos, si no, sin duda estaría usando mis SIMPAD. De hecho, la necesito para jampear a la cloud, la necesito porque no tengo 0 tier acá en la Mac, por cuestiones de seguridad no puedo montarle VPN extrañotas. Bien, cambiemos rápidamente, entonces díganme a qué capa nos estamos yendo, muchachos. ¿En qué capa estamos? ¿A cuál vamos? Capa 4. Bien, capa 4, discutiblemente 5. 4 es transporte, 5 es sesión. Podríamos hacer una discusión filosófica si TCP en particular tiene capa de sesión o no. Vamos a dejar en que no, nos vamos a quedar en capa 4. Vamos de 3, capa IP, capa de red, a 4, capa de transporte. Acá esto es como repaso de lo que ya conocemos, un poco el mapeo, ¿no? Recordemos que en realidad en el modelo TCP, lo super recontra simplifica a estas capas que vemos acá, que es Data Link. Bueno, este en realidad es el modelo OSI, este es el modelo TCP, que es NETWORK INTERFACE, a veces se llama NETWORK ACCESS, INTERNET TRANSPORT Y APPLICATION. Estas son todas las capas del modelo OSI, no las vamos a repasar. Pero sí, acá estamos bastante cómodos en capa de transporte, capa 4. Algo interesante que a nosotros nos gusta recalcar y tratar de ser prolijos con la identidad de paquetes, como bien sabemos estamos en tiempos que hay que ser muy prolijos cuando hablamos de identidades, en este caso no sería otro tipo de entidad, sino identidad de conjunto de bytes. Cuando hablamos en capa 2 hablamos de tramas, cuando hablamos en capa 3, acá le ponen paquetes y no me gusta, hablamos de datagramas, porque paquetes sería más genérico, cuando hablamos en capa 4, donde estamos ahora hablando de segmentos y de mensajes, y en capa de aplicación hablamos de pedazos de data. Esto ya lo habíamos visto, un poco las características, pero vamos más allá de las comparaciones, me gusta que acá nos vamos a empezar a encontrar con cosas que se quieren nuevas. Vamos a encontrar que en los protocolos de transporte TCP, hay estos tres, este tuvo un inicio de SSTP, un inicio como decir, "ah, SSTP nos va a venir fantástico, va a poder hacer telefonía sobre IP, porque es una especie de cosa intermedia entre TCP y UDP, nadie lo usó, así, guac". Directamente se implementó de una manera distinta a nivel de aplicación, a él le dio bolilla porque había que, básicamente, otro protocolo a la altura del stack, que está a la misma altura de TCP y UDP, así que había que implementar unas capas de transporte, capas de transporte de cada endpoint, de nodos finales, y realmente no he visto mayor uso de SSTP. Pero está bueno, vamos a volver sobre esto, está bueno el ejercicio porque nos permite desacoplar conceptos que por ahí nosotros los consideramos acoplados, como por ejemplo, orientado a conexión y corrección de errores, en realidad no necesariamente están acoplados, sí, ambos están en TCP, pero no necesariamente son intrínsecamente, tienen que estar intrínsecamente juntos, así que bueno, ya vamos a volver. Algunas características de TCP, es orientado a conexión, IP es orientado a conexión. ¿Y IP? ¿Cuando un nodo IP quiere comunicarse, enviar un datagramo, otro nodo IP tiene que establecer antes una conexión? No. No, ok. Es muy "love and peace", es decir, yo le tiro el datagramo a la red, la red hace el best effort para llegar al destino, llegará o no llegará, en general, sabemos que tiene mucho éxito en que llegue, pero nada te lo asegura, no sabes si el otro está o no está, insisto en el esfuerzo por preguntarte, iniciar una comunicación con el otro. TCP vamos a ver que hace mucho esfuerzo para que eso sea así, por eso lo hace orientado a conexión. ¿Orientado a conexión es stateful o stateless? Stateful. ¿Por qué? Sí, orientado a conexión. ¿Qué significa y por qué? ¿Nos podrás explicar? Porque la conexión requiere guardar ciertos tipos de datos, que eso vendría a ser el estado. Entonces, si tenés conexión, es stateful. Bien, está muy bien. Ahora, ¿y cuáles serían esos estados? Está muy bien, yo tengo que recordar algo de, yo sé que esto es una conexión, si tenés el caso de IP, como bien sabemos, son datagramas cuyo problema empieza y termina en el datagrama, así lo consideran de hecho los routers en el medio. También el stack, todo el stack de IP, el problema empieza y termina en el datagrama, ya está. Sin embargo, acá hay una conexión, yo tengo que recordar, memorizar, que yo estoy conectado con el otro. Ahora, hay un estado, por eso es stateful. ¿Cuáles serían esos estados de un protocolo orientado a la conexión? Simplificado, ¿no? Muy, muy simplificado. ¿Han visto máquinas de estado cuando han visto programación o algoritmos? Sí. OK. Tratemos de describir cuáles serían los mínimos requerimientos de un protocolo orientado a la conexión, cuáles serían esos estados. Si vos tuvieras que escribirlo y decir, bueno, estamos de un estado X y vamos a saltar a un estado próximo Y, un estado A, un estado B, un estado C, un estado D y potencialmente un estado A de vuelta. ¿Cuál sería ese primer estado inicial? Como estado desconectado, digamos. Desconectado, no conectado, estado cero, ¿sí? Como toda máquina de estado, arranca de un estado en el cual no se ha producido ningún cambio, donde aterrizo por default. Desconectado, un socket desconectado. Inicio de la conexión. Entonces, podríamos, vamos a supersimplificarlo, pero habría un estado intermedio de conectando. Yo lo voy a saltear y voy a saltar derecho, nada más por una simplificación, ya vamos a volver, TCP tiene un montón de estado intermedio. Desconectado o no conectado a conectado. ¿Cuándo puedo transferir datos? ¿Cuándo la aplicación puede transferir datos? ¿Cuál es el requerimiento? Cuando está conectado. Cuando está conectado. Entonces, ahí está fundamentalmente lo stateful. Yo voy cambiando de estado y tengo, no puedo desde la nada empezar a transferir datos. Es decir, estoy desconectado. Ahí van los datos. No, pará, pará, estoy orientado a conexión, papi. Desconectado, establece la conexión. Estamos conectados, estamos en el otro estadio de estado, en el otro circulito de estado, conectados. Ahora sí transferimos datos. Y después, esto es para siempre, listo, nos conectamos para siempre. Ya no nos podemos conectar nunca más. ¿Cuál sería el próximo estado? Solicitar desconexión. Ajá. Solicitar desconexión sería la acción, el verbo. Y el estado sería desconectado, ¿no? Que en realidad es equivalente al estado no conectado, en principio. Puede tener algunas cositas distintas. Vamos aquí a TCP. Yo lo estoy de vuelta, súper simplificando. Es solamente para mostrarles que saltar de un estado a otro requiere esfuerzo, requiere intercambio de paquetes. Algo que, como bien sabemos en IP y venimos súper recalcando, que es stateless, que no es orientado a conexiones, sencillamente porque vos mandás el datagrama. Sí, mandás, plim, y allá va. No hizo falta hacer el esfuerzo de ver si está el otro, si el otro me quiere recibir, si les parecen bien los parámetros de conexión que yo le he pasado, ya estamos de acuerdo, hicimos el handshake, estamos conectados, bueno, podemos transferir datos. Hay como una burocracia de manejo de estados, ¿no? Entonces, importantísimo ese concepto. Veamos otro del... Sí, vamos. ¿Puede ser que esto sea lo de three-way handshake? Vamos a ver. Podrían ser any way, podría tener muchos, vamos a ver por qué son tres. Sí, sí. Three-way handshake es lo que te lleva a un estado desconectado, un estado conectado en el caso de TCP. En el caso de otros protocolos con orientado a conexión, podría ser distinto. Por ejemplo, TLS, que es un protocolo de aplicación, tiene como siete mensajes, porque hay criptografía involucrada, y además acá no hay criptografía, acá es simple, es, vamos a ver por qué es ese three-way handshake. Acá hay algo interesante, la unidad de transporte es el byte en el caso de TCP y no es el mensaje, como ahí... Esto es lo que a mí me gusta describir como una manguera, ¿no? Porque es como que TCP tiene que hacer un trabajo para hacerle ver ese socket del lado origen contra el lado destino que tiene una manguera continua y que este puede inyectar un stream potencialmente infinito de bytes y se modela para la aplicación ese socket como una manguera, donde la unidad de transporte es el byte, no el mensaje. Eso tiene fuertes implicancias, porque por ahí uno lo considera, pero imagínense que ustedes estuvieran haciendo un... tienen que modelar un protocolo de aplicación para SQL, ¿todos conocemos SQL? ¿Sí? ¿Lo hemos visto en alguna parte de la carrera? Sí. Sí, por favor. Ah, bueno, bien. Imagínense que yo quisiera que el protocolo para esta base de datos, le llamo umsql, por "um" SQL, yo puedo hacer pipelining de queries. ¿Qué es pipelining? ¿Alguien me explica? Es poner un set de queries que se vaya ejecutando una después de la otra. O una cola de espera, digamos. Bien, cerca, cerca. En realidad yo puedo enchularle varias a la vez. No tengo que hacer... No tengo que enviar, esperar que se ejecute. Enviar, esperar que se ejecute. Enviar, esperar que se ejecute. Sino que sencillamente yo es como que le enchulo cuatro queries a la vez, y después puedo seguirle poniendo y el otro me va respondiendo, quizás me ha respondido tres queries, quizás yo le enchulo ocho más, una más, y el otro me va respondiendo, y para eso voy a necesitar ponerle identidad a esas queries. Si el protocolo, como es el caso de TCP, es orientado a bytes y no a mensajes, ¿cómo hago yo para diferenciar? ¿Esta es la query 1, esta es la query 2, esta es la query 3? Puedo confiar en que si la aplicación, y esto es importante que lo atendamos, yo soy la aplicación que implementa ese protocolo, que está implementando una aplicación, porque un protocolo es una aplicación, como podría ser SQL. Y yo hago lo siguiente. Sockets, ¿no? Sockets conectados entre ambos lados. Origen, destino. Acá estoy, origen. Le mando. Es pipeline, así que tengo permiso para mandar varias queries SQL a la vez. Le mando la query 1, la query 2, la query 3, la query 4, y se lo escribo en cuatro veces distintas en el socket. Hago un write con el query 1, hago otro write con el query 2, hago otro write con el query 3, y otro write con el query 4. Yo tendré la garantía del otro lado de que quien lee el servidor de base de datos, cuando lea, va a hacer read, ah, mira, acá viene la 1, read, ah, mira, acá viene la 2, acá viene la 3, acá viene la 4. ¿Qué les parece? Existirá esa garantía de que, si yo escribí cuatro veces acá, desde un paquetito de 500 bytes, otro paquetito de 600, otro de 700 y otro de 200, del otro lado cuando leen, leen uno de 500, uno de 600, uno de 700 y uno de 200. ¿Será así? No, pueden llegar a ser ordenados. No, no, TCP asegura que estén ordenados. No es el problema del orden, en este caso. Que es otro de los aspectos acá, eh, ojo. Entrega ordenada de bytes. La pregunta es otra, que es más sutil. Si yo escribo 500, 600, 700, 800, para hacerlo simple. Del otro lado, del socket, del server de base de datos, hace un read. ¿Hará un read de 500 y le garantiza el protocolo que le dé solo 500? ¿Después viene a hacer otro read y saca 700? ¿Después viene a hacer otro read y saca 800? ¿Tiene esa garantía? ¿Sí? ¿Qué les parece? Si el protocolo es garantizado a bytes. ¿Cuál es la unidad de comienzo y fin que se preocupa el protocolo? El byte. Entonces no les dan ninguna garantía de mensaje, pero ninguna. Es más, si ustedes pensaran que esa implementación sobre TCP funcionaría, se les haría torta. ¿Por qué? Porque bien puede ser que si vos escribiste suficientemente rápido del lado de origen, imagínate, mandaste 4 queries y mandaste 500, 600, 700, 800, y como bien sabemos, 500 más 600 es 1100. Entra en un solo y capaz que logró enchularlo en un solo MTU, acá llegaron 1100. Entonces cuando lee el server, lee 1100. Y de pronto se encuentra, ¿cómo hago yo para diferenciar que el query este tiene el query 1 y el query 2? El protocolo no me da esas marcas de comienzo y fin de paquete. No existe tal cosa a nivel de aplicación en TCP. En TCP es un flujo continuo, una manguerita infinita continua de bytes. Si yo tuviera que resolver ese problema, ¿qué tendría que hacer? A nivel de aplicación. Ahora estoy programando el protocolo es el UMSQL. Usar algún flash, no sé. Bien, ok. Voy a tener que yo arreglármelas para que cuando yo escriba esta secuencia de bytes, tenga algún marcador de comienzo y fin para que el otro lo pueda levantar. Ahí gRPC es la implementación más interesante de todo eso, que es Global Remote Procedure Call o Google RPC, que de alguna manera encapsula, pero es a nivel de aplicación. Ya vos definís un protocolo a nivel de aplicación, dado que protocolo de transporte no te asegura esas marcas de inicio y fin de escritura, vos tenés que a nivel de aplicación construirle y poder decir, bueno, si este es todo un mensaje que está encapsulado y tiene esta marquita de comienzo e inicio, pero cuando entra en la manguera infinita va la marquita, va la data, va la otra marquita. ¿Se entiende? Eso es cuando yo tengo un problema que no es un problema típico de HTTP, que estoy descargando algo, por ejemplo, que estoy cargando una página HTML, que si viene una parte que se quiebra justo en el medio de un tag y después viene el resto y viene bien, el navegador lo va a completar correctamente, sino que es el problema cuando yo quiero paquetizar a nivel de aplicación. Bueno, todo esto es para describir, y esto es súper importante, somos muy malos cuando no se... va, muy malos, muy estrictos cuando no se ha comprendido esto. TCP siempre orientado a bytes, contra la aplicación es una manguera continua de bytes. Después veremos cómo se las arregla porque ahora tiene un dolor de cabeza TCP. ¿Cuál es el dolor de cabeza que tiene TCP? Desgraciada aplicación que quiere una manguera continua de bytes y abajo que tengo. Yo soy TCP, capa 4. Arriba tengo... Y IP, es orientado a... Datagramas. Datagramas, sí, y la desgraciada aplicación está queriendo una manguera continua de bytes, así que yo tengo ahora el dolor de cabeza de hacerle creer a ambas patas de la aplicación, los sockets de aplicación que tienen una manguera infinita de bytes, y abajo que me dan para elaborar una cosa paquetizada. Entonces, yo, y esa es la parte más importante que tiene resolver TCP, si se quiere, es cómo convertir, cómo hacer una adaptación, y acá va contra mis amigos electrónicos, una adaptación de impedancias entre... Arriba algo que quiere una secuencia continua de bytes, y abajo que tengo cosas paquetizadas, que ni hablar que son continuas, que sabemos que se pierden, que pueden llegar desordenadas. Entonces yo, papi TCP, tiene que encargarse de modelar un flujo de bytes continuos hacia la aplicación, sobre una red que es paquetizada y que es sin garantía de entrega. Esa es fundamentalmente la misión que tiene TCP. Entrega ordenada. La entrega ordenada se refiere siempre, pensemos esto nuevamente que es a nivel de socket de aplicación. TCP te asegura que vos, si mandaste hola mundo, no vas a recibir mundo hola. Si mandaste hola espacio mundo, vas a recibir quizás ho, la espacio m, ndo, undo. Pero va a venir ordenado, no hay garantía de que venga en el mismo momento, pero ordenadito va a venir. Control de flujo, ya vamos a ver sobre esto cuando nos metamos un poco más. Pero en general, ¿qué significa control de flujo? ¿Qué pasa por ejemplo si yo fuera bueno cantando rap y diera esta clase de TCP como un rapero así? Porque este TCP tiene control de flujo, entonces después envía los paquetes cuando no lo va a hacer, no va a hacer la fragmentación para poder, pero no... ¿Ustedes me escucharían? Iría muy rápido. ¿Cómo? Iría muy rápido. ¿Muy rápido para quién? Para la que escuchamos. No para la red, ¿no? Para la que escucha. Entonces control de flujo es, más allá de esa analogía medio ridícula, es entender que quien transmite no puede empujar más de lo que el otro puede recibir, y tiene que poder leer y entender cuánto está recibiendo el otro, y el otro bien puede decirle "no, para, para, para". El TCP receptor le dice al TCP transmisor "para, para, que todavía no he desbuffereado lo que me has enviado antes". Y el TCP transmisor tiene que parar, porque si no se para, ¿qué va a ocurrir? Pierde el dato. Sí, se va a mal utilizar la red, porque el transmisor envió, hizo todo el esfuerzo de la red para llegar, y este estaba, por decir, con el buffer lleno, y se va a desperdiciar. Entonces, el transmisor tiene que poder de alguna manera leer y saber cuánto el receptor está dispuesto a recibir y modular su transmisión en base a eso. Ese es control de flujo. No tiene nada que ver con entrega ordenada, que no tiene nada que ver con corrección, es sencillamente entender cuál es el grado, la capacidad de recepción del otro lado, y adaptarme a eso. Súper importante. Y control de congestión, en realidad, es, vamos a ver bastante de eso en TCP, es tratar el TCP de entender cómo la red puede estar congestionada, porque como bien sabemos, la red nos feedbackea. Habíamos visto un tipo de mensaje "source quench" de ICMP, pero ya no está usado, entonces, es responsabilidad del TCP darse cuenta, así como, a ver, che, cómo estamos viviendo hoy, darse cuenta que esta red huele y congestiona, vamos a ser un poco más cuidadosos, vamos a bajar ritmo de transmisión, o, fuaa, esta red está fantástica en este momento, démosle fruta. Esa diferencia entre TCP, TCP tiene una manera heurística, que significa no determinística, sino va viendo eventos que ocurren con su stream, vamos a ver qué tan bien está pasando el stream que yo estoy tratando de enviar, y en base a eso se autofeedbackea para decir "ah, acá aparece haber congestión". ¿Sí? Bien. Todo esto lo vamos a ver en detalle, pero ahora como para introducción. Bueno, entremos entonces al TCP. Varios puntos de esto ya lo hemos hablado y visto. Hemos, básicamente, movido un stream de bytes, acá hay un concepto que se llama espacio de secuencia. Esencialmente, espacio de secuencia es TCP numerando lo que la aplicación le escribe, si la aplicación le escribió, por ejemplo, ¿ustedes tienen permitido a nivel de aplicación? Mi gatita, ahí está. Un segundito. Ustedes vieron cómo son los gatos con sus esclavos humanos. Un segundito. Ahí me ayudó Claudia. Se pone loco, y nota el pie. En particular, la desgraciada se pone loca cuando estoy hablando con otra persona, así por mí, y empieza a requerir, "dame bolilla, soy yo, soy yo". Bueno, ahí vamos, volvemos. Espacio de secuencia, entonces. Lo tienen que ver como una especie de ranurado a nivel de byte. Y acabo la pregunta. ¿Ustedes, como usuarios de aplicación de un socket TCP, un socket que hicieron un... ¿recuerdan cómo era para obtener un socket? Un socket de tipo stream, y después le mandan una syscall que es conecta, a destino, le pasan la IP del puerto, imaginemos que se conectan, y después ustedes obtienen un file descriptor, el cual básicamente, un descriptor de archivo, el cual ustedes escriben bytes. Y a ustedes ahí le pueden escribir un byte, o le pueden escribir un mega, si quieren. Y obviamente pueden seguirle escribiendo, pueden hacer, por decir, un while write en el descriptor del socket, le voy enchulando bytes. Entonces, el espacio de secuencia es como TCP modela esto, y básicamente es un concepto súper simple. Empezamos desde cero, vamos a ver que eso es mentira, no es cero literalmente, pero conceptualmente pensémoslo como cero. El momento en que me escriben, nada, el momento en que se estableció la conexión estamos en cero, y si la aplicación me escribió en 10, yo muevo el space de secuencia en 10, y lo voy moviendo de acuerdo a la cantidad de bytes que me escribió la aplicación, y esto me sirve a mí para medir también cuántos bytes he logrado pasar con éxito al otro lado. Es súper importante porque siempre hay como, el espacio de secuencia es como que tiene offsets o marcadores en juego, ¿no? ¿Cómo empezaría? Imaginémonos de un estado conectado, un socket conectado, que estamos en cero, espacio de secuencia en cero, la aplicación me escribe mil bytes, por ejemplo. Entonces yo muevo, tengo mil bytes de problema ahora, como TCP. ¿Por qué es mi problema ahora? ¿Cuál es el problema que tengo que resolver de estos mil bytes de espacio de secuencia? Yo como TCP, ¿qué problema tengo que resolver ahora? Google, hello. Yo soy TCP, la aplicación escribió mil bytes, y los tengo acá en el buffer de transmisión de TCP, estos mil bytes. ¿Qué problema urgente tengo que resolver yo como TCP? Los guardo, los mil bytes, esperamos un rato, qué sé yo, los dejamos ahí, mañana hacemos algo con esto. Total, tengo memoria de sobra. Habría que ver cómo enviarlo, o sea... Mi problema urgente es, tengo que sacármelo de encima. ¿Y qué es sacármelo de encima? Enviárselo al otro lado. Si esa es mi misión, es transmission contra protocol por algo de la T. Si tengo la T, así como si fuera Superman, acá la T dibujada en el pecho, de TCP, yo tengo que resolver el problema de transmitir esto, de enviarlo. Entonces, cuando el espacio de secuencia de todo lo que ya he transmitido coincide con el actual, le hace más simple, no tengo ningún nada buffereado para enviar, tranquilo, yo TCP al menos del lado, de este lado transmisor. Pero apenas la aplicación escribe, yo tengo todo este problema, y mi problema ahora es puchear, empujar estos bytes que le lleguen al destino, que el destino me confirme que le han llegado bien. Ese es mi problema ahora, y ahí tengo esa discorriente, tengo el problema de rascarme la cabeza y decir, bueno, ¿cuál es el MTU que tengo? ¿Cuál es el PMTU que tengo? ¿Entran estos 1000 en los PMTU? Típicamente sí, porque el PMTU es 1500, entonces listo, le pongo la cabecerita, puerto, origen, puerto, destino, entonces lo vamos a ver los puertos, y se lo mando a IP, y vamos a ver, yo tengo que confirmar que eso llegue, porque yo como TCP, a mí me pusieron la estampa de que yo hago entrega asegurada, entonces tengo que estar seguro de que llegue a otro lado, sin errores y qué sé yo, entonces no es que escriba esos 1000 bytes como IP, que se hacía muy del pistola y lo escribí y listo, lo enviaba en su datagrama, sino que yo tengo que pelear por esos 1000 bytes. Entonces, el espacio secuencia es una manera ordenada en que TCP mantiene estos offsets, estos punteros de, ok, estamos acá, la aplicación escribió esto, yo he logrado puchear esta parte, nada más, me falta puchear esto, apenas me vayan confirmando el lado receptor que llegaron bien, yo sigo pucheando, pero es como que hay varios, ¿se entiende esto? ¿no? Varios offsets, ¿no? Hay un punto de arranque que es todo lo que he logrado confirmar que llegó bien y que ya no es problema mío, porque el otro lado me confirmó que llegó ok, ya no es problema más mío, pero yo tengo cuánto está en vuelo, cuánto ha potencialmente escrito la aplicación, yo tengo que jugar con ese espacio secuencia y como es y siempre, obviamente, mi objetivo final es colapsar los espacios de secuencia, mejor dicho, un solo espacio de secuencia, pero colapsar estos offsets, tal que todo lo que recibió el otro lado sea exactamente todo lo que ha escrito hasta ahora este lado. Y esa es la misión del TCP. Y cuando esos se separan, es mi momento de empezar a empujar esos bytes al otro destino, a debufferearlos, ¿no? Los bufferea mi buffer de transmisión que está en el kernel, parte de TCP, y yo tengo que tratar de empujarlos para que el otro lado me confirme y ya estar seguro que pasa. Esa es la misión del TCP. Sí, vamos. Había dicho del PMTU, ¿eso es problema de TCP o de IP? En realidad de los dos, es interesante. Nosotros lo vimos en IP, vimos que IP colaboraba con LEF=1 en caso de IPv4, pero es TCP quien tiene que ajustar el tamaño del segmento. Y a USA y P, TCP va a intentar, esto es lo que hace TCP, imagínense que no ha tenido feedback previo contra el destino S. Pueden estar muchos hops hacia adelante, ¿no? Varios, como bien sabemos, la Internet tiene al menos, no sé, por decir algo, 10 hops hasta los host de Estados Unidos, 15 hops hasta Europa. Si hay muchos hops hacia adelante, yo no sé cuál va a ser el MTU. Pero yo arranco sin memoria previa, yo TCP y digo, ups, me acaban de escribir 10.000 de aplicación, ¿no? La aplicación dice un write de 10.000 bytes. ¿Cuál es el mejor feedback que tengo? Yo tengo que empezar por algo. ¿Por cuánto empiezo? No tengo PMTU aún, no tengo path MTU. ¿Cuál va a ser un valor razonable para empezar, como para no molestar al stack IP acá local? Si me escribieron 10.000 y bueno, empecemos con 2.000, ponele. ¿O no? ¿Empecemos con 500? ¿No? ¿O sí? ¿Para cómo definís qué número? ¿Qué número tengo para mirar? Yo TCP. TCP, como bien sabes, no, recordamos que los implementadores de TCP hicieron un firulito con la separación de capas estricta y dijeron, no, no, no. Acá para que esto performe, los protocolos de distinta altura tienen que ser capaces de ver hacia abajo y hacia arriba para optimizarse. Y acá TCP puede hacer una trampita, ponerse una lupa y mirar hacia abajo y decir, "Upa, no tengo un tamaño razonable. A ver qué tamaño razonable puedo ver en algún lado como para que esto empiece a andar más o menos bien, sin fragmentar, sin subutilizar la red". ¿Qué usarían ustedes? El de la propia red. ¿Qué es eso? ¿Qué es el de la propia red? El MTU que te he definido con el router. Ah, casi. El MTU, no, pero va bien. El MTU de la interfase por la cual yo obtengo la ruta de destino, ¿no? Yo voy a tener muchas rutas. El caso más simple, típico, el de siempre, un host finado con un solo gateway. Veo el dispositivo que me saca el gateway, leo el MTU. Fíjense que estoy leyendo desde el capa 4 una propiedad de capa 2, de capa de enlace. Pero esto es para ayudarle a la capa 3, para no mandarle verdura a la capa 3. Decirle, "Ah, yo soy TCP, vos capa 3, vos podés 64K y me escribirán 10K. Listo, te mando 10K tu problema capa 3". No, no, no, para. Yo me fijo abajo tuyo, abajo de IP, me fijo, "Vos tenés 1,500 abajo IP". Voy a ser gentil, te voy a armar un datagrama que a vos IP te va a hacer que no fragmentes. Fíjense, ¿no? Capa 4 mira capa 2 para hacer que la capa 3 no fragmente. Entonces, hay como una, hay muchos de estos shortcuts, de estos atajos de implementación, de optimización que usa TCP. Todos los atajos de TCP IP. Bien, entonces, ahí vamos. ¿Y qué usarían entonces como resultado? 1,500, ponles y fueron a Ethernet. Y ahí van, de esos 10K que me escribió la aplicación, van los primeros pedacitos de 1,460. Porque tenemos de los 1,500 que entran en Ethernet, le roba 20 cabeceras IP4 y vamos a ver la mínima, la típica. Y vamos a ver que TCP también tiene una cabecera de 20, casualmente. Entonces me quedan 1,460 de esos 10K que me escribió la aplicación. Yo del espacio de secuencia de 10K, que es mi problema ahora, yo tomo los primeros 1,460, corto, recorto, y los meto en un datagrama que entra justito, así tipo Chef, tipo gourmet, entra justito en los, IP le pone su cabecera y entra justito en los 1,000 de Ethernet. Y allá va, prendo una velita, mentira. Y allá va, y ahí es donde entonces sí puedo encontrarme con que un router en el medio tenga una depresión de MTU, porque, por ejemplo, tiene un túnel IPIP, como mencionaba Diego, y no pasa 1,500, el próximo, el próximo capa de link tiene, voy a redondear, típicamente no es así, 1,400. Entonces no va a pasar, ¿y qué ocurre? Ya lo hemos visto. El router descarta, envía un ICMP de regreso, y lo que es interesante es que, ¿a quién afecta ese ICMP? ¿Capa de enlace, capa de red o capa de transporte? La capa de red. Es leído por la capa de red porque dice "MPS capa de red", pero ¿quién actúa en consecuencia? Recuerden, yo fui TCP, no de manera naive, puse un mensaje, digamos, corté mi 1,460 de datos y terminé armando algo para que el datagrama final de IP fuera milky. Y allá fue, se encontró con una depresión de MTU, se descartó, y vino un ICMP de FIBA diciendo "Ah, este no pasó, el próximo MTU en el camino que tengo es 1,400". Vino por ICMP, por lo tanto sí, claro, lo tomó la capa 3, pero ¿quién lee eso y quién actúa en consecuencia? ¿Quién se optimiza? ¿Quién aprende eso, en definitiva? Y la capa 2 ahí. ¿Capa 2, capa de enlace? ¿Ethernet cambia ahí? No, bueno, o sea, si la capa 1. ¿What? Físico, le cambiamos el voltaje al cable de red. Me estoy confundiendo. Sí, no, no hay problema, no hay problema. Fíjense, no, pero está bien, el ICMP viene de regreso con una información importante, la información que acarrea es "Ah, milky no, milky no, no pasó, este milky no pasó, en realidad te estoy informando que tenés 1,400 en el camino". Entonces sí llega la capa 3, pero eso pasa a TCP, y ahora TCP en vez de decir en una forma naive "Y milky, porque es la Ethernet que yo tengo", dice "Ups, acá me vino un mensaje ICMP, yo tengo que memorizar que hacia ese destino tengo 1,400 en el camino". Entonces yo TCP, en vez de volver a intentar enviar estos milky, ¿qué hago? Lo hago más chico. Lo hago más chico, lo hago de 1,360 más cabeceras a 1,400 porque ya he aprendido que en la red hay eso. Entonces, la respuesta es ICMP que acarrea la info importante de que hay una depresión de MTU con 1,400, la recibe ICMP, pero la lee TCP para adaptarse a ese PMTU. Y ahí responde tu pregunta respecto del PMTU. Es TCP en definitiva quien va memorizando esos PMTU y va actuando en consecuencia. Pero fueron descubiertos con la ayuda de la capa de red, con ICMP. Seguimos. ¿Se entendió? Perfecto. Marvelous. Bonici. Confiable y con entrega ordenada. ¿Podría ser confiable y con entrega desordenada? ¿Podría ser no confiable y con entrega ordenada? - Sí, yo creo que podría ser no confiable. - A ver, ejemplo. Primero, definimos confiable. Confiable se refiere acá de que lo que leo es lo que se escribió. Si yo logro leer este pedazo de secuencia de bytes, si dice hola mundo, no dice, perdón, si dice hola, no voy a leer aló, voy a leer hola. ¿Sí? Hola. Perdón, te interrumpí. ¿Me decías que podría ser con entrega no ordenada? - No, decía que tal vez si la entrega es no confiable, podría igual ser ordenada. Igual después me puse a pensar que si es no confiable, puede ser que justo los, digamos, los bytes donde te dice el orden sea la parte no confiable. - Está bien, no, pero está bien, bien podría ser. De hecho, SSTP tiene algunas características, así que este protocolo está como en el medio de WP y TCP. Bien podría ser que vos tenés una entrega confiable. Es decir, si vos leíste algo, no tiene ningún bit flipado en el medio. Si no va, si el otro pedazo escribió hola mundo, vos no vas a leer conejitos mundo. ¿Sí? Vos vas a leer hola. Pero bien podría ser, no es el caso de TCP, que sea confiable sin entrega ordenada. Imagínate, hola, como mundo estás. ¿Sí? Bueno, bueno. Es el garantido de que los bytes que se escribieron fueron sin fliparse, pero no había entrega ordenada. Es una locuración teórica poco práctica, pero lo que estoy tratando de generar en ustedes es romper estos conceptos, porque se ayudan para hacer un protocolo razonable en la vida real, pero en realidad son conceptos distintos. El hecho de que estar seguro de que hay un checksum o algo que me protege este pedazo de data para estar seguro de que es leído tal cual se escribió. Y otra cosa es que venga ordenado. Bueno, TCP tiene todo en entrega ordenada. ¿Por qué? Porque TCP trata de emular esto que vengo insistiendo, que es el modelo de una manguera continua y potencialmente infinita de bytes. Algunas otras características de TCP que tiene un ACK positivo y no tiene ACK negativo. ¿Alguien me puede un poco explicar qué podría significar eso? Y qué es ACK. ACK es por ACK Knowledge, que significa OK, básicamente. Y que es positivo y no negativo. Eso tiene una serie de condiciones de diseño que están súper interesantes de discutir, que es lo que vamos a discutir ahora. Porque se confirman los dataramas que llegaron o sea, las partes que han llegado y no las que faltan. Mensajes. Segmentos. Perdón que te corrija, pero por esto que somos medio stock cuando hablamos en las distintas alturas del protocolo. No, sí, está perfecto. A esta altura son segmentos. ¿Por qué? Porque TCP justamente en la secuencia, en el espacio de secuencia, lo que hace es segmentarlo. Como bien sabemos de cuando incluso íbamos a la primaria, los segmentos tienen una rayita de comienzo y de fin. Entonces TCP, el problema es enviar estos segmentos de un lado a otro. Entonces, bien. Entonces, me estabas diciendo, con lo del ACK positivo. Sí que enviamos, por ejemplo, el segmento 1, 2 y 3 y encima me llegó el 1, me llegó el 2 y el 3 no me llegó. Mandamos el 4. Y como no me llegó el 3, para enviar algo que no me respondieron, que llegó entonces, creo que hay un tiempo de espera y lo vuelve a enviar a partir del último no confirmado, digamos. Buen resumen eso. Y hay una razón, ¿cuál les parece a ustedes puede ser la razón para el otro lado no largarse a decir, "Che, no me llegó el 3". ¿Está claro que... No hay que llegar tarde. Bien, ok. Además yo no sé si existió o no, hay una cuestión hasta causal ahí. Yo, el lado receptor, imagínense, recibo 1, 2, qué sé yo si existe el 3, no le puedo decir "Che, ¿y el 3?" Capaz que el 2 terminó, por este momento. Hay otra condición en la cual sí podría ser activo el lado receptor y decir "Che, el 3, el 2 vino horrible, no pasó el checksum". Yo leo el 1, obviamente como es confiable, hago chequeo de checksum, todos los bits correctos, papá, yo iba. Levanto el 2 y falló el chequeo de checksum. Y yo del lado receptor ahora estoy en un problema, así "Che, le aviso a la origin que esto llegó flipado". Qué malo, el ordenador de TCP, ¿qué le hubiera costado hacer que los receptores de TCP le dijeran a los transmisores de TCP "Che, llegó mal esto", para que "Ay, no más rápido, se corrija". Lo que hace, lo descarta y como nunca le llega el acuce de recibo positivo después de cierto tiempo lo reenvía. ¿Y por qué hicieron eso y no hicieron lo que yo le estoy proponiendo ahora de hacer un rápido acuce de no correcto, NAC? Supongo que es más pesado para la red. A ver cómo sería ese escenario. Sí, porque hay que intercambiar más información. ¿Y qué consecuencia final tendría? Pensémoslo en escala. Ahora estamos con la lupita en dos puntitos, que tiene el problemita chiquitito de que "Uh, me llegó este segmento con los bits flipados" y TCP por diseño no lo va a NACear, no hace nada del lado receptor, lo descarta completamente, no envía un ICMP, no envía nada, nada, nada. Lo descarta. Así. Y se queda calladito. Como bien vos decís, hay alguien aquí que está cuidando, está cuidando a la red. ¿Y por qué está cuidando a la red al hacer eso? Muy bien lo que decís, lo que quiero es que exploremos un poquito más por qué le está cuidando a la red. ¿Qué pasaría si de pronto, imaginemos, en una porción de la red que puede estar implementada, por ejemplo, por canales de wireless, que sabemos que tienen mucho más bit error rate que un canal wired, de pronto ocurre que en camino, los datagramas que acarrean segmentos de TCP empiezan a fliparse sus bits al pasar por acá. Pero no tenemos dos puntitos, pero tenemos todos unos pedazos de internet de un lado a otro y pasan todos con los bits flipados. ¿Y IP no hizo nada al respecto de eso? ¿Entre paréntesis? ¿No tenía un checksum IP? ¿Qué protege el checksum? ¿Las cabeceras? Nada más que sus cabeceras, es súper contra egoísta, IP dice no, yo te garantizo los primeros 20 bytes, lo que viene después no es problema mío, yo no te voy a proteger con mi checksum. Y eso es por diseño. Ok, entonces, volviendo. Tenemos una zona de la red ahí en el medio y tenemos un poco de internet, ¿qué pasa si la red empieza a flipar? Entonces, volviendo. Tenemos una zona de la red ahí en el medio, que tiene un montón de rayos cósmicos, interespaciales y demás, y los bits se flipan y llegan todos rotos al otro lado. ¿Qué ocurriría si todos los TCPs de este otro lado se enojaran y dijieran "Knack, knack, knack, knack". Tendrían que enviar más mensajes que harían que congestionarían más a una red y provocando quizás un nivel de error más alto. Y esto quizás sería un ciclo y cada vez se realimenta más. ¿Por qué? Porque más congestión, más potencialidad de error, más TCP de los otros lados que tienen problemas. Entonces, básicamente, el internet no existiría hoy. Si hubieran decidido poner "knack". ¿Por qué? Porque la realimentación de "che, hay un problemin" hace que el problemin sea peor aún. Entonces, súper importante. Fíjense, a nosotros nos gusta mucho no solamente pasear por el protocolo, sino entender cuáles son los criterios de diseño y entender por qué se eligió uno u otra cosa. En este caso, el hecho de decir "no, acá no hay knack, es todo hack". Y ser un problema del origen, entender así y decir "che, nunca me llegó el hack por esto y bueno, acá lo vamos a reenviar". Nunca tengo que hacer algo, ¿no? Tirar el problema al lado transmisor. Justamente, de vuelta, para cuidar un recurso preciado que habla poco, que es la red. Bien. Retransmisión. Todo esto nos lleva a lo que acabamos de decir. Retransmisión, si hay "hack timeout". Volviendo al ejemplo que mencionaba aquí el compañero, Lucas, decíamos si tenemos 3 segmentos como problema, yo envío el 1, recibí el ok del 1, yo, lado transmisor, ¿no? El segundo segmento, recibí el ok del segundo segmento, envío el tercer segmento, clic, clic, clic, clic, sabemos que no hay knack, clic, clic, clic, clic, y bueno, espero un tiempo razonable, ya vamos a ver qué sería razonable, y lo reenvío, ¿sí? Cruzando los dedos, entre comillas, de que finalmente se me hackea el 3. Recordemos que la misión de TCP es empujar espacios de secuencia. Si el receptor hackea espacios de secuencia, ya no es más problema del transmisor. Y ese es el objetivo, yo tengo que empujar esto que me escribió la aplicación, para que el otro lado me vaya hackeando todo esto, y yo igualar el cursor, si se quiere, sobre ese espacio de secuencia de lo que me hackeó el lado receptor, con lo que yo tengo para transmitir. Si los dos son iguales, no hay nada que hacer. Si los dos son desiguales, yo tengo que empujar esto para hacer que el otro venga. Y ahí viene un poco también el concepto de ventana deslizante, que a veces habla el TCP, es como que sobre este espacio de secuencia, el problema es nuevos bytes que se van sumando en el espacio de secuencia, y cómo yo tengo que lograr que el otro lado me vaya hackeando para asegurarme que esos bytes han pasado al otro lado. ¿Quedó claro más o menos eso? ¿Sí? Sí. Bien. Vamos entonces. Nuevamente tratando de resumir. Ahora quizás volviendo hacia una visión general. Básicamente el CP es lo del circuito virtual, lo que yo defino como esta manguera infinita de bytes. Como es orientada a la conexión, tiene un momento de establecimiento, de uso y de fin de la conexión. Para eso vamos a ver cómo son estos handshakes que alguien mencionó, estos tres mensajes de handshake. Existen los hacks como mecanismo para poder, el lado receptor, a su lado transmisor, ir diciéndole que sea recibido. Lo que es interesante es, a mí me gusta mucho, y lo hablamos recién, cómo TCP hace esta adaptación de una gestión de I/O hacia la aplicación, que tiene que ser una, es una gestión de input/output orientada a bytes, y hacia abajo, hacia la red, una gestión de input/output orientada a paquetes. Entonces, cómo TCP se tiene que agarrar la cabeza y tratar de convertir este continuum de bytes en algo que se corta en paquetes, que son los datagramas que después yo puedo enviar vía IP. Vamos a ver en gestión de la aplicación, cómo se ve hacer la aplicación. Creo que todos a esta altura ya conocemos lo que son los ports. Básicamente, como para dar una idea más formal, así como IP nos permitía identificar nodos en una red, los ports nos permiten identificar procesos dentro de un nodo. Si yo llego con un IP hasta un nodo, y subo, si llego con IP, capa 3, y quiero subir a capa de transporte, en transporte, detrás de un punto de acceso a transporte, un socket, siempre va a haber una aplicación. Entonces, la manera de identificar que es una u otra aplicación a quien yo tengo que entregarle este segmento, va a ser a través de los puertos. Ustedes ya lo conocen, hay puertos bien conocidos, 180 para HTTP, 443 para HTTPS, 22 para SQL, que permiten, son como puertos ya por, de hecho están en RFC, que definen lugares donde el lado pasivo espera que alguien se conecte. Y ya sabemos que a un 180 me voy a conectar para HTTP, a un 443 para HTTPS y a un 22 para SQL. Y aparece el concepto de apertura activa o pasiva, que está muy relacionado con el hecho de que sea orientado a conexión. ¿Alguien me puede, más o menos, comentar qué se imagina que es apertura pasiva o activa de un socket, en este caso, de un punto de acceso? Porque esta es gestión de I/O hacia la aplicación, que son sockets. ¿Qué es la apertura pasiva y qué es la apertura activa? La apertura pasiva puede ser cuando uno está escuchando en el socket solicitudes, por ejemplo, en las servidores web, que escuchan las solicitudes de las otras computadoras que se conecten, y ahí devuelve algo en relación a lo que suceda en el puerto. Y la activa vendría a ser el navegador solicitando al puerto 80, por ejemplo. Bien, buenísimo. [VOCES INTERPUESTAS] Contame. Uno sería del lado del cliente, puede ser, y otro del lado-- Vamos a dejar el tema del cliente-servidor, que típicamente cliente es activo, servidor es pasivo, casi por naturaleza. Pero vamos a posicionarnos más del lado de TCP, de que sea pasivo o activo. Pasivo es, yo abro este punto, un socket, y me pongo a escuchar. Típicamente es un servidor, el 99% de los casos es un servidor, ¿sí? Que dice, OK, yo me pongo a escuchar acá, y sí, tengo un montón de hermosas páginas web para servir en el 80, pero es pasivo, porque yo no es que salgo a buscar al cliente, "Eh, muchacho, ¿cuándo te venden?" ¿Sí? No, sino que abro el 80 y me pongo acá a escuchar a que alguien haga apertura activa. Alguien con un browser, le apunte a mi IP, puerto 80, y se conecte, y ese sí va a hacer una apertura activa, porque ese pide el socket e inmediatamente intenta conectarlo, activamente intenta conectarse al destino. Entonces, el TCP tiene un comportamiento distinto si es una apertura pasiva o una apertura activa. Hace cosas distintas. Que influyen en cómo se mueve cada uno de los lados en esta máquina de estados que referíamos cuando arrancamos esta charla, ¿no? De cómo estoy. Por ejemplo, fíjense que aparece acá otro estado interesante. Nosotros, cuando arrancamos esta charla, hoy simplificamos en estado, ¿no? Conectado, conectado, desconectado. Ahora, la apertura pasiva, ¿tienes estado? ¿Cómo describieron ustedes cuáles, si tuvieran que describirme los estados de un lado de apertura pasiva? ¿Cuáles serían los estados esos que transitamos? Esperando conexión. No conectado, ¿no? Es decir, cero. Vos pediste el socket, nada. No está, nada, ¿no? De hecho, si ustedes recuerdan la interfaz del socket, yo pido un socket y en realidad lo puedo mover hacia activo o pasivo, ¿no? No, no es que cuando yo pido el socket le digo "Che, voy a pedir un socket activo", no. Yo pido un socket. Ya vemos si lo hacemos activo o pasivo. Entonces, de no conectado, he pedido el socket, estoy listo, y entonces, ¿cuál es el próximo estado? Para un lado de apertura pasiva sería... Escuchando. Escuchando, "listen", ¿sí? Me quedo ahí, "pling", y ahí ese, bien puede ser ese mi estado infinito, si soy un website no popular, ¿no? Que nadie se conecta a mi socket, ¿no? Yo sencillamente me quedo ahí, "pling", esperando conexión. ¿Cuándo saltaría de estado? Yo del lado este, del lado que hice la apertura pasiva. Cuando alguien me solicite iniciar una sesión. Una sesión, recordemos que es conexión, sesión es capa 5, ¿no? Protocolo de transporte. Una conexión. ¿Una conexión? ¿Sí? ¿Una conexión? Entonces, sí, hay que hacer del otro lado una apertura activa, cuya máquina de estado salta de no conectado a conectándose. Se intenta conectar a mí, entonces ahora sí ya saltamos de escuchando a conectando, o quizás iniciando conexión, o respondiendo a un inicio de conexión, en estado intermedio, y si todo va bien con el handshake, ahí "rapidito", entre comillas, "rapidito", salta a un estado conectado. Cuando ambos lados logran estar en ese estado conectado, recién pueden intercambiar datos. Bien. Entonces, es importante, fíjense qué distinto, qué asimétrico es una apertura pasiva de una apertura activa. Bien, eso era, eran algunos aspectos de, recuerden TCP/4 tiene arriba aplicación, y abajo tiene red. Entonces veíamos cómo se comportaba hacia la aplicación, gestión de I/O hacia arriba, hacia la aplicación. Gestión de I/O hacia la red, justamente esto es lo que tanto venimos insistiendo, de que transforma un stream continuo de bytes en segmentos finitos, que tienen un comienzo y fin, que terminan siendo datagramas, datagramas de IP. Bueno, y acá ya hemos estado hablando de cómo hace la gestión, pero ya vamos a volverse a estos aspectos de la transmisión y demás. Un aspecto interesante también es el control de flujo en transmisión, y acá viene un concepto, le vamos a decir hola, al concepto de ventana. Vamos a ver que TCP maneja dos ventanas, y vamos a ser bastante molestos en cómo hace el manejo de estas dos ventanas. Entender cuál es la diferencia, en cómo se ayudan y qué significan. Ventana de recepción, es una ventana de terminística, que la escribe el lado que dice, mirá, yo tengo esta cantidad de bytes disponibles para que vos envíes. ¿Qué lado será? ¿El lado receptor o el lado transmisor? El que dice eso. Casi que dice ventana de recepción, lado RX, en bandeja, lado receptor. Y es, estrictamente, es el tamaño del buffer de recepción. Es así, literalmente. Típicamente una implementación de hoy en día, por más memoria que tenga, yo no sé cuántos sockets vamos a tener, podríamos tener ustedes bien. Tenemos 200, siempre andamos con 200 millones de tabs abiertos con cada página web que puede tener una conexión para el HTML, otro para el AJAX, otro para descargar imágenes, otro para descargar updates. No puedo asumir que puedo usar toda la memoria para los buffers, con esto quiero decir que los buffers de recepción y de transmisión son relativamente chicos para los tamaños de memoria RAM que hoy manejamos. Andan en el orden de los 32K, 64K, no es mucho más. Imaginemos 32K, que es un número bastante típico. Entonces yo tengo, yo, lado receptor, acaban de conectarse, yo le digo al lado transmisor, "tenés, tengo 32K para recibirte". Porque acaba de establecerse la conexión, socket conectado, y en el kernel, hay un espacio de 32K. ¿Quién va a ir llenando ese buffer de 32K? Como todo buffer tiene alguien que lo llene y alguien que lo vacía. ¿Quién llena ese buffer de 32K? ¿Y el transmisor, digamos? El lado transmisor, estamos hablando del lado receptor, ¿no? Yo como el lado receptor del TCP, ese buffer está dedicado a vos, transmisor. Quiere decir que vos lo que me vayas enviando, yo lo voy a ir poniendo en este buffer. Ejemplo, recuerden que son 1460, no son 1500, por el tema de las cabeceras, yo lo voy a simplificar en 1500. Imaginemos aproximadamente entonces, 1500 pedazos de data que van llegando acá, ¿sí? Vamos a redondearlo más fácil, vamos a redondearlo en 1000. Tenemos 32K, vamos a redondearlo en 1K, en 1024. Tenemos 32K, viene un segmento de 1K, ¡pling! Ahora tengo 31K disponibles, yo le digo al lado receptor, le corrijo y digo, ojo que ahora tengo 31K. El lado transmisor lee eso, entiende que tiene 31K, sigue enviando, por ejemplo, a pedazos de 1K, manda otro K, ahora tengo 30K y yo le digo al otro lado, mirá, tenés 30K. ¿Sí? Y esto va, siempre hay un juego ahí en que el lado receptor le va diciendo, mirá, yo tengo este espacio de buffer para recibirte en este momento y el lado transmisor tiene que entender eso y tiene que obviamente obedecerlo. Si el otro lado me dice que tiene 10, tiene 1K de espacio de recepción, no le voy a mandar 2K, tengo que obviamente, obviamente tengo que obedecerlo. Bien. Entonces, dijimos, quien llena el buffer de recepción es el lado transmisión. ¿Y quién vacía el buffer de recepción? El receptor. No me hagan trampas, muchachos. Es como diciendo, ¿quién implementa el transporte? Y el transportador. ¿Quién vacía el buffer de recepción? Está muy relacionado con el diálogo que tenemos frente a nosotros. Si el buffer de recepción lo está llenando el lado transmisor, a medida que van llegando los segmentos se van poniendo acá y se va dejando listo, de vuelta son pedacitos, esa manguera infinita de bytes, son justamente secuencias de bytes, que se van, plin, plin, plin, plin, acá ordenando en el buffer de recepción. ¿Y quién me lo limpia el buffer de recepción? El protocolo TCP. Ah, ah, no. El reboot de la máquina me lo limpia. Uy, se llenaron los buffers de recepción. Control, alt, del, de. ¿Quién debería ser ahí el que esté recontra despiartito, así, no como ahora, sino así recién levantadito a la mañana o a la siesta, a la hora que sea que nos estamos exportando en esta época de pandemia, y decimos, uf, vamos a limpiar este buffer de recepción. ¿Quién es el que tiene que estar ahí atento y despejando buffer de recepción? Porque si yo me olvido de limpiar el buffer de recepción, ¿qué va a pasar? Vamos por ese camino y después volvemos sobre el quién. ¿Qué pasa si acá nadie atiende este buffer de recepción y seguimos con esta secuencia? Imaginemos que el lado transmisor tenía para transmitirnos un mega. Yo tengo 32K del lado receptor. El lado transmisor está siendo secuencial, ya vamos a ver cómo eso se hace realmente. Está mandando de un K siempre, ¿no? Un K, ok. Blink, act, a un K. Está todo divino acá en la red. Estamos en una LAN local, súper divino todo. Pero dice ventana 31, ventana 30, ventana 29, ventana, sigue pasando el tiempo, ventana 4K, el otro le escribe un K, ventana 3K, el otro le escribe un K, ventana 2K, ventana 1K, ventana, ventana 0. Y el otro lado del transmisor, pobre, tenía como un mega para transmitir, el cual solamente pudo enchublarle 32K hacia el otro lado. Porque esto 32K está con buffer lleno, básicamente, del lado receptor. ¿So? ¿Quién limpia el buffer receptor? El sistema operativo, no sé. Lo descarta, lo limpia, cada tanto pasa la... para escribir el protocolo de transporte. O la aplicación que va consumiendo. ¡Ah, bueno! Un protocolo de transporte no existe en sí mismo. Existe para brindar un servicio a la aplicación. Entonces la aplicación, la que tiene que vaciar, ¿cómo una aplicación vacía un buffer recepcion? De una manera elegante y difícil de decir que la aplicación está haciendo un... La aplicación vacía el buffer recepcion haciendo un... como en los crucigramos, le ponemos ahí en palabra en inglés, cuatro rayitas. Aplicación vacía buffer recepcion cuando hace qué. ¿Cómo interactúa la aplicación contra un socket abierto, contra un file descriptor? ¿Cuáles son las cosas que empieza a hacer una aplicación contra un socket? Dump, puede ser. Dump no existe. Como syscall, una syscall. Yo te doy un... hiciste connect, socket, hiciste connect, se te conectó, está todo divino, tenés un file descriptor, que es un numerito. Pero vos tenés una interfase de un SDK, el socket SDK, Berkeley socket SDK, pero en definitivo, en el caso de los sistemas que estamos acostumbrados a programar, es un descriptor, y ya un descriptor, ya sea de archivo o de socket, ¿qué le hago? Como flujo de I/O de la aplicación. ¿Qué puedo hacer contra un descriptor de archivo? ¿Qué le hago típicamente? ¿Puedes abrir una aplicación, muchachos? Si abren un archivo, ¿qué le hacen? Mmm... No, no. Open file. Y después, ¿qué hago? Imagínense que, no sé. Un read. Bien, read or write. Sí, read or write. Sí. Sí, read or write. Así de simple, read or write. El lado de transmisor de TCP, lo que hace es responder a los writes de la aplicación, y la aplicación, la desgraciada aplicación, me hizo un write de un mega, yo tengo nada más que 32k de ventana al otro lado, entonces le voy empujando a pedacitos de un k, que es lo que me permite la red abajo. Pero tengo que tener la colaboración de la aplicación, ¿y cómo entonces? Ahí vamos de vuelta a la preguntita de cruciagrama. La aplicación despeja el buffer de recepción haciendo un... Read, read. Read. Muy bien. Ahí. Sucesivo read, en realidad. Y es ahí donde la aplicación tiene que estar despierta. Tendrá algún mecanismo de señalización, tal que la aplicación se la avise, y acá hay algo en este descriptor, viene la aplicación, y yo estoy teetralizando, estoy rápido despeja, ¿por qué es importante que la aplicación rápido despeje y haga algo con esos bytes? No sé, qué sé yo, depende de qué está haciendo. Por ahí la aplicación es un browser que está recibiendo un flujo HTTP con contenido HTML para desplegarlo en su navegador, entonces el browser irá consumiendo este socket y irá... y mostrándoselo a ustedes. O puede ser un descargador de torrents que tiene que despejar este buffer y empezar a escribir los segmentitos de datos, los pedacitos, los chunks de datos en el torrent de ese ISO que se están bajando, ¿no? Pero ¿por qué es importante que la aplicación esté despierta y vaya rápido y haga el read? Y para la parte de la aplicación, para completar el archivo que esté utilizando, como decíamos, el HTML, no se lo puede mostrar en completo. Porque si no hay el... no me sale la palabra... el que está enviando los datos no puede seguir enviando, digamos. ¿Por qué no puede seguir enviando? Porque se llena el buffer. Ok, bien, ahí está, se llena el buffer. De todos los aspectos estos que habíamos visto en la diapo 3, ¿de cuál estamos hablando ahora, entonces? Control de flujo. Control de flujo, exacto. Y ese es cómo se materializa el control de flujo en el caso de TCP. En el caso de TCP, el control de flujo es justamente esto, y quien implementa eso byte a byte es la ventana de recepción. Y de hecho hay momentos en donde el TCP receptor puede decir, listo, acá la aplicación se fumó, no vino a leer más el socket, yo tengo 32K super lleno, lado transmisor, sorry, ventana cero, pling, ventana cero, listo, vos tendrás gigas para transmitirme, pero acá hay ventana cero, y no podés transmitirme, los perderías, porque acá no tengo más para buffer, no te voy a inventar espacio de buffer acá. Buffer se llenó, chao, ventana cero, no transmitís más. Cuando la aplicación se despierte y empieza a consumir, entonces yo te voy diciendo, ah, bueno, acá hay un K de buffer, 2K de buffer, 15K de buffer, entonces eso te va a dar a vos, lado transmisor, el, entre comillas, permiso de poder seguir enviándome segmentos para yo seguir acá, ¿no? ¿Se entiende eso? Entonces es como que la aplicación es importante siempre, en cualquier escenario, que despeje esos buffers, porque eso le da oxígeno al lado receptor para poder decir al lado transmisor que sigue enviando. Estamos... Sí, quería solamente un ejemplito, un último ejemplo. Yo tengo una duda. Decime. Y eso de que le envíe, le dice que me queda 30 espacios o 20 espacios, ¿cómo se llama o qué le está enviando? Ventana Reception, RxWindow. Ahora lo vamos a ver, justamente la clase que viene con los cerebros de usted y los nuestros, mucho más fresco. Vamos a ver estos dos, que acá vamos a hablar mucho de todo eso. Pero es Ventana Recepción. Y se escribe en la cabecera, Visión al futuro, fíjense que acá está la cabecera y acá hay Windows. Acá se escribe literalmente. Estos bits, 16 bits que tengo acá, son para decir al otro lado, tengo cero o tengo... Les miento, 32K, porque fíjense que dice que tiene 16 bits nada más. No, siempre 32K, miento, me auto-miento. Tengo 32K, tengo 10K, tengo cero. Y eso lo escribo yo del lado receptor en ese. Fíjense que no es opcional, forma parte de la cabecera TCP y tiene un lugar dedicado para siempre en la cabecera. Porque es muy importante decir al lado receptor cuánto espacio, cuánto permiso tiene para escribir. Solamente quería ponerles, quería... ¿Sabes que los buffer tienen como máximo 65K? Lo vamos a ver, en realidad hay un truco, hay un hack de TCP para aumentarlo. Hay un hack. Que le agrega unos bits más para poder llevarlo, se pueden llevar hasta a un giga. Pero es con extensiones de TCP. Es como que imaginate, dijera, "Uh, muchachos, nos quedamos recortos". Bueno, pongamos una opción a TCP y le aumentamos la cantidad de bits. Es como que vos hubieras... Se llaman literalmente... ¿Cómo es? Ventana Amplified... No me sale la palabra, pero es una extensión de TCP que te permite expresar una ventana más grande. Pero está muy bien tu observación, está muy muy bien. 2A16. Pregunta. A ver si tengo acá... No tengo ningún USB. Ah, sí, acá tengo. Tengo un disco USB 2.0. No se lo estoy tratando de vender ahora. Esto no es un add-on, un bonus track de venderle este disco viejo USB 2.0. USB 2.0 ¿a qué velocidad graba? ¿Más o menos? ¿Más o menos? 600 GB por segundo. 1K por segundo, 10 MB por segundo. ¿A cuánto graba más o menos? 500 MB. No, no, eso es USB 3.0. Bueno, no importa. Vamos a asumir que este es tan viejo que graba a 1 MB por segundo. 1 MB por segundo. Vieja, asaso. Ustedes capaz que nunca han visto... En realidad es USB 3.0, pero estamos teatralizando el problema. Imagínense que acá yo tengo algo que es capaz de escribir acá a 1 MB por segundo. ¿A cuánto labura una red de 1 GB? De vuelta, números grandes, números, potencias de 10. 1 GB por segundo. ¿A cuánto puedo escribir de un lado a otro? 1024 MB. No, eso sería 1 GB. Es a 1000 MB por segundo. Sí, es 1 GB. ¿Cuántos bytes? Redondeando, súper redondeando. ¿Cuántos bytes? Que están más cerca a lo que... Que están en la misma unidad que yo les digo acá, 1 MB por segundo. ¿A cuánto escribe una red de 1 GB por segundo? ¿A cuánto mueve, no a cuánto escriba? ¿A cuánto puedo mover? ¿Bit a byte? 125. Bien, ahí estamos, 125 MB por segundo. A mí me encanta porque por suerte tenemos la excusa de las cabeceras, así que podemos redondear muy tranquilo a 100 MB. Si tengo 1000 MB, ¿cómo puedo redondear? ¿Cómo puedo redondear? Así que podemos redondear muy tranquilo a 100 MB. Si tengo 1000 MB por segundo, en vez de dividirlo en 8 lo divido en 10 porque le agrego ahí burocracia cabecera, qué sé yo. Y de paso, no, y sobre todo me da un orden de magnitud. Entonces, 100 MB por segundo, 100 veces más rápido que esto. Esto, 1 MB por segundo. ¿Qué pasa si yo tengo lo siguiente? Imagínense. Conecto esto a un host destino. Plac, lo conecto a un host destino. Host destino acá. Host origen y yo quiero copiar algo directamente al disco, sin pasar por el disco interno del host, porque bien puede no tenerlo, puede ser, no sé, puede ser un, como un router WRT, esos que tienen USB. Yo quiero escribir directo desde acá a ese disco. Estoy haciendo un backup remoto, ¿sí? Estoy tomando, no sé, archivos que yo tengo en esta máquina satélite y lo estoy backupeando a este disco que lo tengo conectado a este host remoto. Las especificaciones son tales que tengo discos súper rápidos acá porque tengo una máquina como la que tenemos hoy, como NessSD, que tienen literalmente gigabytes por segundo de ancho de banda, de disco, acá, en esta máquina. Pero tengo una red que es capaz de mover a lo que acabamos de discutir, 100 megabytes por segundo y tengo un destino final, este, que escribe a un megabyte por segundo. ¿Qué les parece? ¿Cómo se van a comportar, si yo tuviera que medir el promedio de velocidad que yo absorbo en la red, cuál va a ser ese promedio de velocidad? Y el más lento. ¿Cuánto? El más lento, uno. Bien, bien. No va a ser 100, yo voy a tener como bursts, ¿no? Voy a ver como... De 100, porque la red no tiene un potenciómetro para trabajar a uno o a 100, siempre trabaja 100 megabytes por segundo, o un gigabit por segundo. Entonces yo voy a tener estos bursts. ¿Y quién va a ser ahí el que va a terminar ajustando y decir "no, loco, esto es divino, acá tenés SSDs, acá tenés una red de 100 megabytes por segundo, pero el destino final es un megabyte por segundo"? ¿Qué va a ocurrir con ese buffer ahí en el medio, ese 32K, el buffer de recepción? ¿Va a estar la mayor parte del tiempo lleno o vacío? Lleno. ¿Por qué? Sí, porque se va a estar escribiendo más rápido lo que se puede ir leyendo. Bien. Y la red le va a ir diciendo "esperá que tengo que terminar de escribir". Muy bien, muy bien. Perfecto, ahí la aplicación no es que sea, no es que sea, que esté "lazy", que esté, ¿cómo es? ¿Qué vendría a ser? No es que la aplicación sea lenta o esté mal diseñada, sencillamente que la aplicación lo que hace es leer del socket y escribir en el disco y no le queda otra que un megabyte por segundo, entonces va a sacar el socket y leer ese disco lento al ritmo que le permita, como muy bien dijeron, el más lento de todos estos. Por lo tanto, este buffer va a estar fundamentalmente lleno. ¿Sí? A medida que la aplicación es capaz de escribir, a medida que el disco le permite decir "ah, bueno, ahí te voy escribiendo más", le va a ir despejando pedacitos de buffer al otro lado del transmisor y eso le va a ir dando permiso al otro transmisor para seguir enviando segmentos. Pero va a ser una cosa medio "varsity", ¿no? No es que voy a llenar de 100 megabytes por segundo, 1 gigabyte por segundo, constante, sino que va a ser... Es más, literalmente sería 1 cada 100. Es como que... Pasa 100 veces... Porque hay una relación de 1 a 100 entre la velocidad del disco y la velocidad de la red, ¿no? Va a haber una cosa así. Si uno tuviera que reflejarlo sería un cosito rápido, un palito que es 100 veces más chiquito que todo el silencio, 99 de silencio, 1 de envío, 99 de silencio, 1 de envío. ¿Sí? Y el que va a estar haciendo, literalmente, buffereando en el medio es el buffer de recepción. Bien, solamente quería dejarles esa idea como para terminar de bajar el concepto de cuál es la misión del control de flujo en un buffer de recepción en un caso como este. Así que, bueno. Muy bien, estamos terminando por hoy, voy a terminar la grabación y, Salvo que tengan alguna duda concreta respecto a lo que hablamos hoy. Yo sí. Sí. Cuando hablábamos de puertos, que dice que los puertos identifican un proceso o servicio. ¿Está limitado un puerto por proceso? O si uno quiere puede tener todo un proceso. No, no, no, es interesante, muy buena pregunta. Es interesante porque típicamente uno piensa que es 1 a 1. Vos tenés un web server atendido en un puerto 80. Pero bien puede ser, en ambos sentidos, bien puede ser que el web server no tenga un solo proceso web server, atiende el 80, atiende el 443 y quizás tenga un puerto administrativo en el 8080, por el cual vos te conectás para administrarlo. Es decir que un proceso puede tener varios puertos. Y a su vez lo opuesto, que es quizás el caso más interesante, un solo puerto con multiproceso. A lo 80 este lo están atendiendo 4 instancias de mi web server. Y ocurre, oh, qué casualidad que yo tengo 4 cores, 4 CPU en esta máquina. ¿Para qué sería eso? ¿Qué les parece? Claro, exacto. Entonces yo, bien puede ser un solo puerto, no digo una conexión, ahí ya es otro problema más complejo. Pero un puerto, por ejemplo, especialmente en modo pasivo, ¿qué ocurriría? Imagínense, somos 4 cerebritos, 4 procesos, atendiendo nuevas conexiones en el 80. ¿Y qué ocurriría a medida que van llegando las conexiones activas del lado cliente? Abre Control 80. Va a un proceso. A un proceso. Y ese se queda dedicado a esa conexión. El proceso A queda dedicado a esa conexión. ¿Qué pasa cuando viene la próxima conexión? Va a otro proceso. Proceso B. Y así se multiplexan las conexiones. ¿Por qué? Porque yo, por ejemplo, he atado cada proceso a un CPU para poder hacer un mejor uso de este hardware. Entonces fíjate que es interesante, ¿no? Puede ser el mismo puerto contra múltiples procesos, o un solo proceso abriendo muchos puertos. El caso más típico, más simple a entender es 1 a 1, pero no necesariamente es así. Perfecto. Gracias. No, de nada. ¿Estamos? ¿Alguna otra duda de lo que acabamos de hablar? ¿Y eso en qué capas estaría, se manejaría? ¿O ya sería algo del sistema operativo, otra parte del sistema operativo? Estamos hablando de la interfaz de transporte de aplicación acá. ¿Sí? El socket, que es esto, el puerto, es en realidad como la capa de transporte, son las reglas que le pone la aplicación. La aplicación tiene que entender puertos para identificarse. Entonces, justo como siempre esta implementación de capas, siempre hay interfaces, ¿no? Porque son como niveles de capas y interfaces justamente la rayita. Estamos hablando literalmente de esto, ¿no? Acá, no sé si se ve el color, justo en el transporte de aplicación. Los sockets están ahí. Entonces, es esa, el contrato, si se quiere, en donde hablamos puertos cuando hablamos de estos huecos que yo hago, sockets, para poder usar este protocolo de transporte, puertos. No sé si eso responde a tu pregunta. Sí, yo como que tengo, se me está empezando a mezclar. Me acuerdo que en un momento me puse a leer, a estudiarlo y ahora como que... Porque, por ejemplo, yo mi navegador, yo lo abro y puedo tener el Google Chrome y el Edge, los dos abiertos y los dos pueden estar utilizando protocolo HTTP y encima los dos pueden tener distintas ventanas abiertas. Y estarían todos usando el mismo puerto. Origen, destino, ¿de qué puerto me estás hablando? Sí, todos están utilizando-- Bueno, ahí no hemos llegado a ese punto, pero imaginate, imaginemos por un momento Bostonets, Brave, Chrome, Firefox, abiertos todos contra Google. Imaginemos que Google te haga una sola IP de endpoint, es decir que es como que Google estuviera en un solo lugar, lo que vos haces cuando haces un ping Google Chrome, eso. Y están todas estas ventanas y tabs de los navegadores contra el mismo destino. Y ahí tendríamos en principio un problema porque se habla de una 5UPLA, que es lo que identifica unívocamente una conexión. Una parte de la UPLA, esa de la 5UPLA, es el protocolo, TCP, listo. Me quedan cuatro grados de libertad. IP destino, recién acabo de decir que me dio una sola. Puerto destino es 443, que es HTTPS. IP origen, yo tengo una sola IP. Entonces, ¿qué me queda como variable para poder diferenciar esta conexión de este tab de navegador, de esta otra, de esta otra, de esta otra, de esta otra? Si tengo toda esta fija. Una sesión. No, no hablemos del mismo término que estamos hablando. No hablemos de sesión, estamos hablando de transporte. Contra la aplicación. Dijimos, IP origen e IP destino. Dijimos, puerto destino que está fijo porque es 443. ¿Y qué me queda libre entonces? El puerto de... Los mis puertos, digamos. Puerto origen, exactamente, claro. Entonces, ese es el juego. Y tu sistema operativo en realidad va a poder diferenciar una conexión distinta de la otra porque tiene la libertad de elegir el puerto de origen cualquiera. No tiene por qué elegir el 80443. De hecho, no elige eso, sino que elige randómicamente dentro de un rango que típicamente está arriba de 1024. Ah, en realidad, por RFC está arriba de 1024. Y va a picotear dentro de los puertos libres que yo tengo, que son, vamos a ver, que son 65K puertos que tengo yo de mi lado. Y cuando voy contra el mismo voy a ir mordiendo distintos puertos justamente para permitir hacer que esta conexión sea distinta a la otra y no sé, entre comillas, confundan la conexión que viene al crome del Firefox. Sencillamente porque justamente como sus puertos de origen son distintos, permite diferenciar una de otra, por más que vayan al mismo destino. No sé si se explica... Sí, perfecto, perfecto. Se me había pisado. Buenísimo, buenísimo, sí. Y también siempre presente la cincupla y dónde vos tenés ese grado de libertad y generalmente lo vas a encontrar en esa posibilidad de definir, de usar un puerto distinto del lado cliente. Bien, buenísimo. ¿Alguna otra duda, muchachos? Bueno, estamos. Gracias por los 29 minutos extras. Pero... [AUDIO_EN_BLANCO]